# Resumo executivo (1 frase)

O pipeline principal √©: executar run.py ‚Üí parse dos argumentos ‚Üí run.py escolhe uma classe Exp_* em exp ‚Üí Exp_*._get_data() chama data_provider() que cria um DataLoader ‚Üí Exp_*._build_model() instancia um modelo de models ‚Üí Exp_*.train() itera batches do DataLoader e chama self.model(...) ‚Üí checkpoints e resultados s√£o salvos em ./checkpoints/<setting>/ e ./results/ ou ./test_results/.
Fluxo detalhado (componentes e conex√£o)

Pontos de entrada e controle
run.py ‚Äî script principal. Voc√™ roda este arquivo com flags CLI. Ele:
Define e parseia todas as flags (dados, modelo, treino, GPU, etc.).
Ajusta args.use_gpu e args.device_ids se multi-GPU.
Seleciona a classe de experimento (Exp_Long_Term_Forecast, Exp_Imputation, etc.) com base em --task_name.
Para cada repeti√ß√£o (args.itr): cria exp = Exp(args), constr√≥i setting (nome √∫nico do experimento) e chama exp.train(setting) seguido de exp.test(setting).
Classe base
exp_basic.py (classe Exp_Basic):
Possui model_dict mapeando nomes de modelos para m√≥dulos em models.
_acquire_device() define o device GPU/CPU e configura CUDA_VISIBLE_DEVICES.
Ao instanciar Exp_*, Exp_Basic chama _build_model() (implementado nas subclasses) e move o modelo para self.device.
Data loading
data_factory.py:
Mapeia args.data para classes de Dataset (em data_provider), ex.: 'm4' ‚Üí Dataset_M4, 'Global_Weather_Station' ‚Üí Dataset_Weather_Stations.
data_provider(args, flag) cria e retorna (data_set, data_loader).
Diferen√ßas por args.task_name:
Para anomaly_detection e classification e global_forecast o batch_size e drop_last s√£o tratados de forma espec√≠fica.
global_forecast usa um MyRandomSampler customizado e train_steps (loop por itera√ß√µes, n√£o por √©pocas completas).
m4 pode ter drop_last=False.
O DataLoader fornece batches como tuples de tensores (ex.: (batch_x, batch_y, batch_x_mark, batch_y_mark)), formando a entrada que os Exp_* esperam.
Experimentos (exp)
Cada arquivo exp_* cont√©m:
_build_model() ‚Äî cria o modelo (via self.model_dict[self.args.model].Model(self.args)).
_get_data(flag) ‚Äî chama data_provider(args, flag).
_select_optimizer(), _select_criterion() ‚Äî otimizador/perda.
train(setting), vali(...), test(setting) ‚Äî comportamento espec√≠fico.
Diferen√ßas principais entre Exp_*:
Exp_Long_Term_Forecast: treino por √©pocas, valida√ß√£o e EarlyStopping, salva checkpoint.
Exp_Short_Term_Forecast: perdas customizadas (MAPE/SMAPE/MASE) e ajuste para M4 dataset.
Exp_Imputation: aplica m√°scaras aleat√≥rias (com mask_rate) e calcula perda s√≥ nos pontos mascarados.
Exp_Global_Forecast: usa train_steps (itera por steps via next(train_loader)), sampler customizado, logging, e valida√ß√µes peri√≥dicas; salva checkpoints periodicamente.
Exp_Classification: ajusta seq_len/enc_in/num_class baseado nos dados, usa CrossEntropyLoss e valida por acur√°cia.
Exp_Anomaly_Detection: treina reconstru√ß√£o e avalia anomalias atrav√©s de thresholding sobre o erro de reconstru√ß√£o.
Modelos
models cont√©m implementa√ß√µes (cada m√≥dulo exporta uma classe Model):
Ex.: Pyraformer.py, Autoformer.py, TimesNet.py, etc.
Exp_Basic seleciona o modelo via self.model_dict[self.args.model].
O contrato: o modelo recebe entradas conforme a task (p.ex. model(batch_x, batch_x_mark, dec_inp, batch_y_mark)), e retorna previs√µes (formato esperado por cada Exp_*).
Utils
print_args.py: fun√ß√£o print_args(args) imprime um resumo leg√≠vel dos argumentos.
tools.py: fun√ß√µes utilit√°rias:
adjust_learning_rate(...), EarlyStopping, StandardScaler, visual(), adjustment() etc.
Checkpoints e outputs
Checkpoints: salvo por EarlyStopping.save_checkpoint em path + '/checkpoint.pth' onde path = os.path.join(args.checkpoints, setting).
Resultados: Exp_* salvam m√©tricas/outputs em ./results/<setting>/, ./test_results/<setting>/, ou arquivos texto (result_*.txt).
Log: Exp_Global_Forecast usa utils.logger.Logger para file logging. Outros impress√µes v√£o para stdout.
Guia passo-a-passo para treinar (recomendado)

Preparar dados

Garantir args.root_path e args.data_path corretos. Ex.: para ETT dataset configure --root_path ./data/ETT/ --data_path ETTh1.csv.
Para WEATHER-5K (Global_Weather_Station), use --data Global_Weather_Station e ajuste --root_path se necess√°rio.
Escolher tarefa e modelo

Escolha --task_name (um dos suportados) e --model (por exemplo Pyraformer, Autoformer).
Ex.: previs√£o longa com Pyraformer:
Para global forecast:
Rodar e monitorar

Observe o stdout e o diret√≥rio ./checkpoints/<setting>/.
Se quiser salvar logs, ajuste --checkpoints para uma pasta com permiss√£o de escrita.
Testar/avaliar

Ap√≥s o treino run.py chama exp.test(setting) automaticamente quando --is_training 1.
Para rodar s√≥ o teste em um checkpoint existente:
Certifique-se de reproduzir exatamente os argumentos que geraram o setting para localizar o checkpoint.
Dicas de debugging e reprodu√ß√£o

Comece com poucos epochs e batch_size pequeno para validar pipeline:
--train_epochs 1 --batch_size 2
Para ver formas e fluxo: coloque import pdb; pdb.set_trace() em pontos do Exp_* (ex.: in√≠cio do train()).
Verifique o setting impresso para confirmar onde o checkpoint ser√° salvo.
Para usar GPU: --use_gpu True. Para multi-GPU: --use_multi_gpu --devices "0,1".
Logs de NaN: alguns Exp_* chamam pdb.set_trace() se torch.isnan(loss) acontecer.
Pr√≥ximos passos que eu posso fazer por voc√™

Gerar exemplos de comandos prontos para cada Exp_* (com valores sugeridos).
Adicionar uma flag --resume no run.py para retomar treino a partir de checkpoint.pth.
Criar um README mais pr√°tico (comandos de uso e checklist pr√©-treino).
Quer que eu:

(A) gere os comandos prontos para cada Exp_* agora; ou
(B) implemente um --resume em run.py e atualize o c√≥digo com testes r√°pidos?
GPT-5 mini ‚Ä¢ 0x
#  WEATHER-5K Benchmark 
## Paper: [WEATHER-5K: A Large-scale Global Station Weather Dataset Towards Comprehensive Time-series Forecasting Benchmark](https://arxiv.org/abs/2406.14399)

## Introduction

The WEATHER-5K dataset is a large-scale time series forecasting dataset containing weather data from 5,672 weather stations worldwide. It is a valuable resource for researchers and developers in the field of time-series forecasting, providing a comprehensive evaluation of various methods and models.
WEATHER-5K dataset consists of a comprehensive collection of data from 5,672 weather stations worldwide, spanning a 10-year period with one-hour intervals. It includes multiple crucial weather elements (temperature, dewpint temperature, wind speed, wind rate, sea level pressure), providing a more reliable and interpretable resource for forecasting.

<p align="center">
<img src=".\asset\Overview.png" height = "350" alt="" align=center />
</p>


:triangular_flag_on_post:**News** (2024.06)  We release the WEATHER-5K as a comprehensive benchmark, allowing for a thorough evaluation of time-series forecasting methods and facilitates advancements in this field.

## Leaderboard of WEATHER-5K benchmark

Until now, we have bnchmarked the following models in this repo:
  - [x] **iTransformer** - iTransformer: Inverted Transformers Are Effective for Time Series Forecasting [[ICLR 2024]](https://arxiv.org/abs/2310.06625) [[Code]](https://github.com/thuml/Time-Series-Library/blob/main/models/iTransformer.py).
  - [x] **Corrformer** - nterpretable weather forecasting for worldwide stations with a unified deep model [[NMI 2023]](https://www.nature.com/articles/s42256-023-00667-9) [[Code]](https://github.com/thuml/Time-Series-Library/blob/main/models/Corrformer.py).

  - [x] **PatchTST** - A Time Series is Worth 64 Words: Long-term Forecasting with Transformers [[ICLR 2023]](https://openreview.net/pdf?id=Jbdc0vTOcol) [[Code]](https://github.com/thuml/Time-Series-Library/blob/main/models/PatchTST.py).
  - [x] **DLinear** - Are Transformers Effective for Time Series Forecasting? [[AAAI 2023]](https://arxiv.org/pdf/2205.13504.pdf) [[Code]](https://github.com/thuml/Time-Series-Library/blob/main/models/DLinear.py).
  - [x] **FEDformer** - FEDformer: Frequency Enhanced Decomposed Transformer for Long-term Series Forecasting [[ICML 2022]](https://proceedings.mlr.press/v162/zhou22g.html) [[Code]](https://github.com/thuml/Time-Series-Library/blob/main/models/FEDformer.py).
  - [x] **Pyraformer** - Pyraformer: Low-complexity Pyramidal Attention for Long-range Time Series Modeling and Forecasting [[ICLR 2022]](https://openreview.net/pdf?id=0EXmFzUn5I) [[Code]](https://github.com/thuml/Time-Series-Library/blob/main/models/Pyraformer.py).
  - [x] **Autoformer** - Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting [[NeurIPS 2021]](https://openreview.net/pdf?id=I55UqU-M11y) [[Code]](https://github.com/thuml/Time-Series-Library/blob/main/models/Autoformer.py).
  - [x] **Informer** - Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting [[AAAI 2021]](https://ojs.aaai.org/index.php/AAAI/article/view/17325/17132) [[Code]](https://github.com/thuml/Time-Series-Library/blob/main/models/Informer.py).


<!-- 
**Note: We will keep updating this leaderboard.** If you have proposed advanced and awesome models, you can send us your paper/code link or raise a pull request. We will add them to this repo and update the leaderboard as soon as possible. -->

## Benchmarking results 

The results are reported at 4 different prediction lengths: 24, 72, 120, and 168, where the input length is 48. 
<table>
  <colgroup>
    <col>
    <col>
    <col style="border-left: 1px solid black;">
    <col style="border-right: 1px solid black;">
    <col style="border-left: 1px solid black;">
    <col style="border-right: 1px solid black;">
    <col style="border-left: 1px solid black;">
    <col style="border-right: 1px solid black;">
    <col style="border-left: 1px solid black;">
    <col style="border-right: 1px solid black;">
    <col style="border-left: 1px solid black;">
    <col style="border-right: 1px solid black;">
  </colgroup>
    <tr>
        <td>Baselines<strong></td>
        <td>Lead Time</td>
        <th colspan="2">Temperature</th>
        <th colspan="2">Dewpoint</th>
        <th colspan="2">Wind Speed</th>
        <th colspan="2">Wind Direction</th>
        <th colspan="2">Sea Level Pressure</th>
        <th colspan="2">Overall</th>
    </tr>
    <tr>
        <td></td>
        <td></td>
        <td>MAE</td>
        <td>MSE</td>
        <td>MAE</td>
        <td>MSE</td>
        <td>MAE</td>
        <td>MSE</td>
        <td>MAE</td>
        <td>MSE</td>
        <td>MAE</td>
        <td>MSE</td>
        <td>MAE</td>
        <td>MSE</td>
    </tr>
<tr>
        <td rowspan="4">ü•á 1st Pyraformer</td>
        <td>24</td>
        <td><strong>1.75<strong></td>
        <td><strong>6.92<strong></td>
        <td><strong>1.83<strong></td>
        <td><strong>7.88<strong></td>
        <td><strong>1.30<strong></td>
        <td><strong>3.58<strong></td>
        <td>61.8</td>
        <td>6930.2</td>
        <td><strong>1.90<strong></td>
        <td><strong>9.72<strong></td>
        <td>13.7</td>
        <td>1391.7</td>
    </tr>
    <tr>
        <td>72</td>
        <td><strong>2.47<strong></td>
        <td><strong>13.03<strong></td>
        <td><strong>2.67<strong></td>
        <td><strong>15.39<strong></td>
        <td><strong>1.52<strong></td>
        <td>4.97</td>
        <td>72.0</td>
        <td><strong>8222.4<strong></td>
        <td><strong>3.76<strong></td>
        <td><strong>33.67<strong></td>
        <td>16.5</td>
        <td>1657.9</td>
    </tr>
    <tr>
        <td>120</td>
        <td><strong>2.77<strong></td>
        <td><strong>16.04<strong></td>
        <td><strong>3.00<strong></td>
        <td><strong>18.95<strong></td>
        <td>1.59</td>
        <td><strong>5.37<strong></td>
        <td><strong>75.1<strong></td>
        <td>8610.7</td>
        <td><strong>4.43<strong></td>
        <td><strong>43.91<strong></td>
        <td><strong>17.4<strong></td>
        <td>1739.0</td>
    </tr>
    <tr>
        <td>168</td>
        <td><strong>2.95<strong></td>
        <td><strong>17.95<strong></td>
        <td><strong>3.20<strong></td>
        <td><strong>21.06<strong></td>
        <td>1.61</td>
        <td>5.56</td>
        <td>76.4</td>
        <td>8773.5</td>
        <td><strong>4.77<strong></td>
        <td><strong>49.97<strong></td>
        <td><strong>17.8<strong></td>
        <td>1773.6</td>
    </tr>
        <tr>
        <td rowspan="4">ü•à 2nd iTransformer</td>
        <td>24</td>
        <td>1.82</td>
        <td>7.49</td>
        <td>1.93</td>
        <td>8.80</td>
        <td>1.32</td>
        <td>3.77</td>
        <td>63.2</td>
        <td>7358.8</td>
        <td>1.99</td>
        <td>10.84</td>
        <td>14.1</td>
        <td>1478.0</td>
    </tr>
    <tr>
        <td>72</td>
        <td>2.60</td>
        <td>14.46</td>
        <td>2.84</td>
        <td>17.5</td>
        <td><strong>1.52<strong></td>
        <td>4.96</td>
        <td>73.2</td>
        <td>8713.3</td>
        <td>4.14</td>
        <td>40.65</td>
        <td>16.9</td>
        <td>1758.2</td>
    </tr>
    <tr>
        <td>120</td>
        <td>2.97</td>
        <td>18.36</td>
        <td>3.24</td>
        <td>22.16</td>
        <td>1.59</td>
        <td>5.42</td>
        <td>76.4</td>
        <td>9192.2</td>
        <td>4.95</td>
        <td>54.67</td>
        <td>17.8</td>
        <td>1858.6</td>
    </tr>
    <tr>
        <td>168</td>
        <td>3.18</td>
        <td>20.64</td>
        <td>3.48</td>
        <td>24.89</td>
        <td>1.64</td>
        <td>5.67</td>
        <td>78.0</td>
        <td>9441.1</td>
        <td>5.36</td>
        <td>62.31</td>
        <td>18.3</td>
        <td>1910.9</td>
    </tr>
    <tr>
        <td rowspan="4">ü•â 3rd Informer</td>
        <td >24</td>
        <td>1.88</td>
        <td>7.51</td>
        <td>1.94</td>
        <td>8.30</td>
        <td><strong>1.30<strong></td>
        <td>3.62</td>
        <td><strong>60.7<strong></td>
        <td><strong>6906.9<strong></td>
        <td>2.01</td>
        <td>10.56</td>
        <td><strong>13.6<strong></td>
        <td><strong>1387.4<strong></td>
    </tr>
    <tr>
        <td>72</td>
        <td>2.75</td>
        <td>14.84</td>
        <td>2.86</td>
        <td>17.24</td>
        <td>1.53</td>
        <td><strong>4.86<strong></td>
        <td><strong>71.5<strong></td>
        <td>8251.4</td>
        <td>4.24</td>
        <td>39.24</td>
        <td><strong>16.4<strong></td>
        <td><strong>1631.4<strong></td>
    </tr>
    <tr>
        <td>120</td>
        <td>3.11</td>
        <td>18.21</td>
        <td>3.25</td>
        <td>21.50</td>
        <td>1.60</td>
        <td>5.38</td>
        <td>75.7</td>
        <td><strong>8504.5<strong></td>
        <td>5.15</td>
        <td>54.31</td>
        <td>18.3</td>
        <td><strong>1720.4<strong></td>
    </tr>
    <tr>
        <td>168</td>
        <td>3.24</td>
        <td>20.24</td>
        <td>3.43</td>
        <td>24.89</td>
        <td>1.63</td>
        <td>5.65</td>
        <td><strong>76.2<strong></td>
        <td><strong>8718.4<strong></td>
        <td>5.26</td>
        <td>58.42</td>
        <td>18.1</td>
        <td><strong>1764.4<strong></td>
    </tr>
    <tr>
        <td rowspan="4">Autoformer</td>
        <td>24</td>
        <td>1.93</td>
        <td>8.64</td>
        <td>2.06</td>
        <td>9.57</td>
        <td>1.42</td>
        <td>3.97</td>
        <td>66.5</td>
        <td>7710.0</td>
        <td>2.26</td>
        <td>12.78</td>
        <td>15.2</td>
        <td>1553.4</td>
    </tr>
    <tr>
        <td>72</td>
        <td>2.72</td>
        <td>15.14</td>
        <td>2.97</td>
        <td>18.38</td>
        <td>1.54</td>
        <td>5.14</td>
        <td>75.4</td>
        <td>9111.5</td>
        <td>4.25</td>
        <td>42.34</td>
        <td>17.8</td>
        <td>1846.7</td>
    </tr>
    <tr>
        <td>120</td>
        <td>3.21</td>
        <td>20.27</td>
        <td>3.34</td>
        <td>23.12</td>
        <td>1.58</td>
        <td>5.73</td>
        <td>79.2</td>
        <td>9143.5</td>
        <td>4.83</td>
        <td>48.88</td>
        <td>18.1</td>
        <td>1868.3</td>
    </tr>
    <tr>
        <td>168</td>
        <td>3.43</td>
        <td>21.71</td>
        <td>3.56</td>
        <td>22.55</td>
        <td>1.64</td>
        <td>5.95</td>
        <td>79.8</td>
        <td>9435.8</td>
        <td>5.32</td>
        <td>61.85</td>
        <td>18.5</td>
        <td>1885.7</td>
    </tr>
    <tr>
        <td rowspan="4">FEDformer</td>
        <td>24</td>
        <td>1.98</td>
        <td>8.45</td>
        <td>2.02</td>
        <td>9.25</td>
        <td>1.36</td>
        <td>3.91</td>
        <td>66.0</td>
        <td>7384.1</td>
        <td>2.13</td>
        <td>11.43</td>
        <td>14.7</td>
        <td>1483.4</td>
    </tr>
    <tr>
        <td>72</td>
        <td>2.87</td>
        <td>16.50</td>
        <td>3.01</td>
        <td>18.70</td>
        <td>1.59</td>
        <td>5.31</td>
        <td>76.2</td>
        <td>8824.8</td>
        <td>4.15</td>
        <td>37.60</td>
        <td>17.6</td>
        <td>1780.6</td>
    </tr>
    <tr>
        <td>120</td>
        <td>3.19</td>
        <td>20.29</td>
        <td>3.36</td>
        <td>23.10</td>
        <td>1.66</td>
        <td>5.71</td>
        <td>79.0</td>
        <td>9143.3</td>
        <td>4.81</td>
        <td>48.86</td>
        <td>18.4</td>
        <td>1848.3</td>
    </tr>
    <tr>
        <td>168</td>
        <td>3.35</td>
        <td>22.12</td>
        <td>3.54</td>
        <td>25.21</td>
        <td>1.68</td>
        <td>5.88</td>
        <td>79.7</td>
        <td>9189.2</td>
        <td>5.01</td>
        <td>53.39</td>
        <td>18.7</td>
        <td>1859.2</td>
    </tr>
    <tr>
        <td rowspan="4">Dlinear</td>
        <td>24</td>
        <td>2.71</td>
        <td>13.82</td>
        <td>2.47</td>
        <td>12.36</td>
        <td>1.44</td>
        <td>4.34</td>
        <td>66.6</td>
        <td>8234.5</td>
        <td>3.09</td>
        <td>21.34</td>
        <td>15.3</td>
        <td>1657.3</td>
    </tr>
    <tr>
        <td>72</td>
        <td>3.55</td>
        <td>23.05</td>
        <td>3.48</td>
        <td>22.85</td>
        <td>1.62</td>
        <td>5.37</td>
        <td>75.0</td>
        <td>9250.8</td>
        <td>4.64</td>
        <td>45.83</td>
        <td>17.7</td>
        <td>1869.6</td>
    </tr>
    <tr>
        <td>120</td>
        <td>3.90</td>
        <td>27.60</td>
        <td>3.89</td>
        <td>27.72</td>
        <td>1.67</td>
        <td>5.70</td>
        <td>77.3</td>
        <td>9510.6</td>
        <td>5.19</td>
        <td>56.22</td>
        <td>18.4</td>
        <td>1925.6</td>
    </tr>
    <tr>
        <td>168</td>
        <td>4.11</td>
        <td>30.38</td>
        <td>4.11</td>
        <td>30.58</td>
        <td>1.69</td>
        <td>5.88</td>
        <td>78.4</td>
        <td>9630.0</td>
        <td>5.48</td>
        <td>61.73</td>
        <td>18.8</td>
        <td>1951.7</td>
    </tr>
    <tr>
        <td rowspan="4">PatchTST</td>
        <td>24</td>
        <td>2.05</td>
        <td>9.26</td>
        <td>2.16</td>
        <td>10.58</td>
        <td>1.40</td>
        <td>4.20</td>
        <td>66.2</td>
        <td>7765.8</td>
        <td>2.19</td>
        <td>12.54</td>
        <td>14.8</td>
        <td>1560.5</td>
    </tr>
    <tr>
        <td>72</td>
        <td>2.82</td>
        <td>16.60</td>
        <td>3.06</td>
        <td>19.96</td>
        <td>1.60</td>
        <td>5.39</td>
        <td>75.2</td>
        <td>9067.8</td>
        <td>4.28</td>
        <td>42.46</td>
        <td>17.4</td>
        <td>1830.5</td>
    </tr>
    <tr>
        <td>120</td>
        <td>3.15</td>
        <td>20.32</td>
        <td>3.43</td>
        <td>24.39</td>
        <td>1.66</td>
        <td>5.79</td>
        <td>77.8</td>
        <td>9452.6</td>
        <td>5.09</td>
        <td>57.29</td>
        <td>18.2</td>
        <td>1912.1</td>
    </tr>
    <tr>
        <td>168</td>
        <td>3.33</td>
        <td>22.54</td>
        <td>3.63</td>
        <td>26.94</td>
        <td>1.69</td>
        <td>6.00</td>
        <td>79.0</td>
        <td>9638.1</td>
        <td>5.51</td>
        <td>65.3</td>
        <td>18.6</td>
        <td>1951.7</td>
    </tr>
    <tr>
        <td rowspan="4">Corrformer</td>
        <td>24</td>
        <td>1.99</td>
        <td>8.21</td>
        <td>2.09</td>
        <td>9.47</td>
        <td>1.38</td>
        <td>3.83</td>
        <td>66.7</td>
        <td>7832.3</td>
        <td>2.19</td>
        <td>12.39</td>
        <td>14.9</td>
        <td>1584.4</td>
    </tr>
    <tr>
        <td>72</td>
        <td>2.74</td>
        <td>15.16</td>
        <td>2.99</td>
        <td>18.40</td>
        <td>1.56</td>
        <td>4.91</td>
        <td>75.6</td>
        <td>9111.7</td>
        <td>4.27</td>
        <td>42.36</td>
        <td>17.8</td>
        <td>1846.7</td>
    </tr>
    <tr>
        <td>120</td>
        <td>3.06</td>
        <td>18.63</td>
        <td>3.34</td>
        <td>22.48</td>
        <td>1.61</td>
        <td>5.56</td>
        <td>78.0</td>
        <td>9477.4</td>
        <td>5.08</td>
        <td>57.13</td>
        <td>18.1</td>
        <td>1915.8</td>
    </tr>
    <tr>
        <td>168</td>
        <td>3.09</td>
        <td>18.69</td>
        <td>3.36</td>
        <td>22.53</td>
        <td>1.63</td>
        <td>5.69</td>
        <td>78.9</td>
        <td>9636.0</td>
        <td>5.34</td>
        <td>61.83</td>
        <td>18.4</td>
        <td>1938.8</td>
    </tr>
    <tr>
        <td rowspan="4">Mamba</td>
        <td>24</td>
        <td>1.98</td>
        <td>8.59</td>
        <td>2.01</td>
        <td>9.52</td>
        <td>1.37</td>
        <td>4.02</td>
        <td>66.0</td>
        <td>7709.5</td>
        <td>2.21</td>
        <td>12.73</td>
        <td>14.7</td>
        <td>1548.9</td>
    </tr>
    <tr>
        <td>72</td>
        <td>2.79</td>
        <td>16.00</td>
        <td>2.90</td>
        <td>18.11</td>
        <td>1.55</td>
        <td>5.11</td>
        <td>75.1</td>
        <td>8863.9</td>
        <td>4.29</td>
        <td>41.88</td>
        <td>17.3</td>
        <td>1789.0</td>
    </tr>
    <tr>
        <td>120</td>
        <td>3.03</td>
        <td>18.47</td>
        <td>3.18</td>
        <td>21.02</td>
        <td><strong>1.58<strong></td>
        <td>5.28</td>
        <td>76.7</td>
        <td>8931.2</td>
        <td>4.93</td>
        <td>52.56</td>
        <td>17.9</td>
        <td>1805.7</td>
    </tr>
    <tr>
        <td>168</td>
        <td>3.16</td>
        <td>19.88</td>
        <td>3.32</td>
        <td>22.53</td>
        <td><strong>1.59<strong></td>
        <td><strong>5.35<strong></td>
        <td>77.4</td>
        <td>8958.8</td>
        <td>5.21</td>
        <td>57.37</td>
        <td>18.1</td>
        <td>1812.8</td>
    </tr>
</table>
**Compared models of this leaderboard.** ‚òë means that their codes have already been included in this repo.


## Usage

1. Install Python 3.8. For convenience, execute the following command.

```
pip install -r requirements.txt
```

2. Prepare Data. You can obtain the well pre-processed datasets from [[OneDrive]](https://hkustconnect-my.sharepoint.com/:u:/g/personal/thanad_connect_ust_hk/EZGm7DP0qstElZwafr_U2YoBk5Ryt9rv7P31OqnUBZUPAA?e=5r0wEo), Then place and `unzip` the downloaded data in the folder`./WEATHER-5K`. 


3. Train and evaluate model. We provide the experiment scripts for all benchmarks under the folder `./scripts/`. You can reproduce the experiment results as the following examples:

```
# Global Station Weather Forecasting
bash ./scripts/weather-5k/iTransformer.sh

```

4. Develop your own model.

- Add the model file to the folder `./models`. You can follow the `./models/Transformer.py`.
- Include the newly added model in the `Exp_Basic.model_dict` of  `./exp/exp_basic.py`.
- Create the corresponding scripts under the folder `./scripts`.

## Citation

If you find WEATHER-5K is useful, please cite our paper.

```
@misc{han2024weather5k,
    title={WEATHER-5K: A Large-scale Global Station Weather Dataset Towards Comprehensive Time-series Forecasting Benchmark},
    author={Tao Han and Song Guo and Zhenghao Chen and Wanghan Xu and Lei Bai},
    year={2024},
    eprint={2406.14399},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
```

## Contact
If you have any questions or suggestions, feel free to contact:

- Tao Han (hantao10200@gmail.com)

Or describe it in Issues.

## Acknowledgement

This library is constructed based on the [Time-Series-Library](https://github.com/thuml/Time-Series-Librar). We sincerely thank the contributors for their contributions.


